{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c3de78-f95d-4e45-9aa0-b5496e7fb765",
   "metadata": {},
   "source": [
    "### Chunknig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552c65c6-81f6-4bf4-8446-91412f7cf84c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('quick', 'JJ'),\n",
       " ('brown', 'NN'),\n",
       " ('fox', 'NN'),\n",
       " ('jumps', 'VBZ'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('lazy', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.chunk import RegexpParser\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize and POS-tag the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "tags = pos_tag(tokens)\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8da2f33-229c-474d-b144-20c49637fda4",
   "metadata": {},
   "source": [
    "#### Notes on RE: \n",
    "- <> Means that what's inside is a Part of Speech tag and not a string.\n",
    "- ? means optional --> $<DT>?$ means optional determiner\n",
    "- $*$ means zero or more occurances --> $<JJ>*$ means zero or more occurances of Adjective\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06cf9048-7fb2-407c-9f05-83cc383be426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               S                                           \n",
      "    ___________________________|________________________________            \n",
      "   |     |            NP               NP       VP              NP         \n",
      "   |     |     _______|________        |        |        _______|______     \n",
      "over/IN ./. The/DT quick/JJ brown/NN fox/NN jumps/VBZ the/DT lazy/JJ dog/NN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a chunking pattern using regular expressions\n",
    "# This example finds noun phrases (NP) like 'The quick brown fox'\n",
    "grammar = \"\"\"\n",
    "  NP: {<DT>?<JJ>*<NN>}   # Determiner (DT) + Adjective (JJ) + Noun (NN)\n",
    "  VP: {<VB.*>}            # Verb phrase (VP)\n",
    "\"\"\"\n",
    "\n",
    "# Create a chunk parser using the grammar\n",
    "chunk_parser = RegexpParser(grammar)\n",
    "\n",
    "# Apply the chunking pattern to the POS-tagged sentence\n",
    "tree = chunk_parser.parse(tags)\n",
    "\n",
    "# Show the chunked sentence\n",
    "tree.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1997f945-7544-47a1-a0c3-cd309b75ca2a",
   "metadata": {},
   "source": [
    "Note that brown is tagged as a noun while it is actually an adjective. So, when using POS, we need to be aware of the errors. But overall, the Chunking operation extracts patterns in the text which can be very useful for extracting information or task of NER. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e4858-cc09-4753-80e9-b1c0d1233245",
   "metadata": {},
   "source": [
    "### Chinking \n",
    "Once the chunking is finished, you might want to exclude the prepositions, or adjectives. This is done by Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23d09dde-4e30-4400-be2c-0fc73dd79060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 S                                          \n",
      "     ____________________________|_______________________________            \n",
      "    |         |     |            NP                              NP         \n",
      "    |         |     |     _______|________________        _______|______     \n",
      "jumped/VBD over/IN ./. The/DT quick/JJ brown/NN fox/NN the/DT lazy/JJ dog/NN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.chunk import RegexpParser\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The quick brown fox jumped over the lazy dog.\"\n",
    "\n",
    "# Tokenize and POS-tag the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "tags = pos_tag(tokens)\n",
    "\n",
    "# Define chunking pattern for noun phrases (NP)\n",
    "# We will include determiners (DT), adjectives (JJ), and nouns (NN), \n",
    "# but we will remove any prepositions (IN) from the chunk.\n",
    "grammar = \"\"\"\n",
    "  NP: {<DT>?<JJ>*<NN>*}   # Noun Phrase: Optional determiner, adjectives, and nouns\n",
    "  NP: {<DT>?<JJ>*<NN> -<IN>}   # Exclude prepositions (IN) from noun phrase\n",
    "\"\"\"\n",
    "\n",
    "# Create a chunk parser using the grammar\n",
    "chunk_parser = RegexpParser(grammar)\n",
    "\n",
    "# Apply the chunking and chinking rules to the POS-tagged sentence\n",
    "tree = chunk_parser.parse(tags)\n",
    "\n",
    "# Show the chunked sentence\n",
    "tree.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "446880ed-e44f-4518-b2d8-9a4e58203672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 S                                          \n",
      "     ____________________________|_______________________________            \n",
      "    |         |     |            NP                              NP         \n",
      "    |         |     |     _______|________________        _______|______     \n",
      "jumped/VBD over/IN ./. The/DT quick/JJ brown/NN fox/NN the/DT lazy/JJ dog/NN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.chunk import RegexpParser\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The quick brown fox jumped over the lazy dog.\"\n",
    "\n",
    "# Tokenize and POS-tag the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "tags = pos_tag(tokens)\n",
    "\n",
    "# Define chunking pattern for noun phrases (NP)\n",
    "# We will include determiners (DT), adjectives (JJ), and nouns (NN), \n",
    "# but we will remove any prepositions (IN) from the chunk.\n",
    "grammar = \"\"\"\n",
    "  NP: {<DT>?<JJ>*<NN>*}   # Noun Phrase: Optional determiner, adjectives, and nouns\n",
    "  VP: {<VB.*><IN> -<IN>} \n",
    "  NP: {<DT>?<JJ>*<NN>}   # Exclude prepositions (IN) from noun phrase\n",
    "\"\"\"\n",
    "\n",
    "# Create a chunk parser using the grammar\n",
    "chunk_parser = RegexpParser(grammar)\n",
    "\n",
    "# Apply the chunking and chinking rules to the POS-tagged sentence\n",
    "tree = chunk_parser.parse(tags)\n",
    "\n",
    "# Show the chunked sentence\n",
    "tree.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d6f1b-ac06-485c-9590-2871d0c00979",
   "metadata": {},
   "source": [
    "### First coding practice: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99bab392-0bcf-4ef2-987f-6b2b162d659d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "import random \n",
    "import re, unicodedata\n",
    "import string \n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import wikipediaapi as wk\n",
    "from collections import defaultdict\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "396f05ea-bda2-401d-a8bf-5f83edb2b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HR.txt', 'r') as file : \n",
    "    raw = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "052d271b-377d-4c21-85a9-bc0eb0a1e798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'human resource analytics is at the intersection of three bodies of knowledge:\\n\\nhuman resource management: sets the meaning and purpose of the analytics. \\n\\ndata warehousing: knowing how to process and store hr data efficiently, automation of  collection of data and cleaning data.\\n\\nstatistical analysis, presentation and interpretation : helps in translating the identified hr  issues into appropriate analyses and communication of results.\\n\\n\\n\\n5 fundamental principles of analytics\\n\\nhr analytics is about metrics and measurement. good metrics definitions, both narrative and formulaic, and their documentation are key.\\n\\na professional and good hr analytics person will have the above bodies of knowledge and know their process and intersection.\\n\\ngood communication and collaborative skills are essential. the in-depth expertise in your organization is likely to exist in hrm. it. and decision support. you will need to collaborate with these groups.\\n\\nthe extent of hr analytics can be vast. having a defined model or framework can help you navigate towards your future efforts. patience is key.\\n\\nwhile quality preparation of metrics is important, the real value is in the analysis and interpretation.\\n\\nspecialists analytics in high demand for hr roles: study\\n\\net bureau -may 14, 2018\\n\\nindia has seen 77% growth in specialized analytics professionals employed in human resource functions in the past five years, as an increasing number of companies are turning to analytics to address workforce planning, skills gap and employee retention, according to a study by professional network linkedin.\\n\\n“the overall business landscape and the changing role of human resource as a more strategic partner in business are driving the rise of analytics in hr. it is imperative for human resource today to give more strategic inputs in business and build a data-driven human resource management function,” said irfan abdulla, director-linkedin talent solutions and learning solutions, india and south asia.\\n\\ncompen'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to lower case\n",
    "raw = raw.lower()\n",
    "raw[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de73e2f-a3e2-4c6a-a9d8-edf5b40b5d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['human resource analytics is at the intersection of three bodies of knowledge:\\n\\nhuman resource management: sets the meaning and purpose of the analytics.',\n",
       " 'data warehousing: knowing how to process and store hr data efficiently, automation of  collection of data and cleaning data.',\n",
       " 'statistical analysis, presentation and interpretation : helps in translating the identified hr  issues into appropriate analyses and communication of results.',\n",
       " '5 fundamental principles of analytics\\n\\nhr analytics is about metrics and measurement.',\n",
       " 'good metrics definitions, both narrative and formulaic, and their documentation are key.',\n",
       " 'a professional and good hr analytics person will have the above bodies of knowledge and know their process and intersection.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the data into sentences \n",
    "sent_tokens = nltk.sent_tokenize(raw)\n",
    "sent_tokens[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b8f6e-866e-473a-b0c7-e7c5461534af",
   "metadata": {},
   "source": [
    "The dictionary remove_punk_dict maps every punctuation to a None value. The once the translate function is called, every character in the text will be run through the translate function and if the character is matched with any key in remove_punk_dict, that character is mapped to None, i.e. will be removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2ae658-d669-4dcf-b2b8-7e255d3fd0f7",
   "metadata": {},
   "source": [
    "The key feature of defaultdict is that it allows you to specify a default value that will be returned when you try to access a key that doesn't exist in the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d5e059-cbf2-46f5-bcce-d8e2a7b0caf1",
   "metadata": {},
   "source": [
    "### What Unicode Normalization Does\n",
    "\n",
    "Unicode normalization is a process that changes a string to a standardized form, which is helpful when dealing with characters that may have multiple representations. For example, some characters can be represented as a single \"composite\" character or as multiple \"decomposed\" characters.\n",
    "\n",
    "### NFKD Normalization\n",
    "\n",
    "The `\"NFKD\"` normalization form stands for **Normalization Form KD (Compatibility Decomposition)**. It breaks characters down into their basic parts. Specifically:\n",
    "\n",
    "1. **Decomposition**: It splits characters with diacritics (like accents or tildes) into separate components. For instance:\n",
    "   - `é` becomes `e` + `´` (an 'e' with a separate accent mark).\n",
    "   - `ñ` becomes `n` + `~` (an 'n' with a separate tilde mark).\n",
    "   \n",
    "2. **Compatibility Decomposition (KD)**: It also converts characters to simpler, compatibility-equivalent forms. For example:\n",
    "   - The symbol `ℌ` (script capital H) will normalize to the standard `H`.\n",
    "   - The ligature `ﬂ` will normalize to `f` + `l`.\n",
    "\n",
    "After normalizing with `NFKD`, if you then encode to ASCII (as the code does), any non-ASCII components (like diacritics) are removed, leaving just the ASCII characters.\n",
    "\n",
    "### Example\n",
    "\n",
    "Here’s what happens when you apply `unicodedata.normalize('NFKD', word)` on different characters:\n",
    "\n",
    "- `\"café\"` → `\"cafe\" + ´`\n",
    "- `\"résumé\"` → `\"resume\" + ´ + ´`\n",
    "- `\"ℌ\"` → `\"H\"`\n",
    "- `\"½\"` (the fraction one-half) → `\"1/2\"`\n",
    "\n",
    "In short, `NFKD` normalization simplifies text by decomposing characters and stripping accents or special markings, which is helpful for creating ASCII-compatible strings.\n",
    "\n",
    "Then we encode each character to ascii and if any character is not known (like the accents), they will be ignored. \n",
    "We then decode back to the 'utf-8', while ingoring the unknown characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4f9aacc-da50-413a-915c-d3f5f8b765cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(x): \n",
    "    remove_punk_dict = dict((ord(punct),None) for punct in string.punctuation)\n",
    "    word_token = nltk.word_tokenize(x.lower().translate(remove_punk_dict))\n",
    "    \n",
    "    #remove ascii  [\"café\", \"résumé\", \"naïve\", \"façade\", \"jalapeño\", \"niño\"] -> [\"cafe\", \"resume\", \"naive\", \"facade\", \"jalapeno\", \"nino\"]\n",
    "    new_words = []\n",
    "    for word in word_token:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "        \n",
    "    # Modify HTML tags\n",
    "    rmv = []\n",
    "    for w in new_words:\n",
    "        text=re.sub(\"&lt;/?.*?&gt;\",\"&lt;&gt;\",w)\n",
    "        rmv.append(text)\n",
    "        \n",
    "    # Add POS tags \n",
    "    pos_map = defaultdict(lambda: wn.NOUN)\n",
    "    pos_map['J'] = wn.ADJ \n",
    "    pos_map['V'] = wn.VERB\n",
    "    pos_map['R'] = wn.ADV\n",
    "    # Define the word lemmatizer \n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemma_list = []\n",
    "    rmv = [i for i in rmv if i] # if i means if it's not NONE or empty list -> remove any empty lists \n",
    "    for token, tag in nltk.pos_tag(rmv):\n",
    "        lemma = lmtzr.lemmatize(token, pos_map[tag[0]])\n",
    "        lemma_list.append(lemma)\n",
    "    return(lemma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7b601da-fcad-4816-a5c1-4172dcd05117",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_map = defaultdict(lambda: wn.NOUN)\n",
    "pos_map['J'] = wn.ADJ \n",
    "pos_map['V'] = wn.VERB\n",
    "pos_map['R'] = wn.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c73624d5-320f-44b3-a35f-51901afa4251",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = nltk.word_tokenize(sent_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11ae1c0b-ab79-4684-b009-b056c4bf55ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_map[res_tag[0][0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4070fbf-7cd5-4259-942d-328a300a51f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('human', 'JJ'),\n",
       " ('resource', 'NN'),\n",
       " ('analytics', 'NNS'),\n",
       " ('is', 'VBZ'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('intersection', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('three', 'CD'),\n",
       " ('bodies', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('knowledge', 'NN'),\n",
       " (':', ':'),\n",
       " ('human', 'JJ'),\n",
       " ('resource', 'NN'),\n",
       " ('management', 'NN'),\n",
       " (':', ':'),\n",
       " ('sets', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('meaning', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('purpose', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('analytics', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tag = nltk.pos_tag(res)\n",
    "res_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4829177-b8d7-41c7-a2dd-d608b5a67d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'human resource analytics is at the intersection of three bodies of knowledge:\\n\\nhuman resource management: sets the meaning and purpose of the analytics.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66f059fa-ae18-4afe-ae16-eb7831070de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['human',\n",
       " 'resource',\n",
       " 'analytics',\n",
       " 'be',\n",
       " 'at',\n",
       " 'the',\n",
       " 'intersection',\n",
       " 'of',\n",
       " 'three',\n",
       " 'body',\n",
       " 'of',\n",
       " 'knowledge',\n",
       " 'human',\n",
       " 'resource',\n",
       " 'management',\n",
       " 'set',\n",
       " 'the',\n",
       " 'meaning',\n",
       " 'and',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'the',\n",
       " 'analytics']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Normalize(sent_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf8f55f-7448-4099-bd4c-ea096aa1d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "welcome_input = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "welcome_response = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def welcome(user_response):\n",
    "    \"\"\"\n",
    "    This is the welcome part of the ChatBot \n",
    "    \"\"\"\n",
    "    for word in user_response.split():\n",
    "        if word.lower() in welcome_input:\n",
    "            return random.choice(welcome_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b4404-a0b0-44d6-ae1c-3f83cc60fb18",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** is a numerical statistic used in text mining and information retrieval to reflect how important a word is to a document in a collection or corpus. The **TF-IDF Vectorizer** transforms a text into a vector of TF-IDF scores, helping represent the text's \"importance\" in a way that can be used for machine learning tasks.\n",
    "\n",
    "#### 1. What is TF-IDF?\n",
    "\n",
    "TF-IDF combines two metrics:\n",
    "\n",
    "- **Term Frequency (TF)**: This measures the frequency of a word in a document. A higher term frequency indicates the word is more representative of that document.\n",
    "  - **Formula**:\n",
    "    \n",
    "  - $ \\text{TF} = \\frac{\\text{Number of times a term appears in a document}}{\\text{Total number of terms in the document}}\n",
    "    $\n",
    "- **Inverse Document Frequency (IDF)**: This measures the importance of a word across all documents in the corpus. Words that appear frequently in many documents (like \"the\", \"is\") receive a lower score, while words unique to specific documents receive a higher score.\n",
    "  - **Formula**:\n",
    "    \n",
    "    $\n",
    "    \\text{IDF} = \\log \\left( \\frac{\\text{Total number of documents}}{\\text{Number of documents containing the term}} \\right)\n",
    "    $\n",
    "\n",
    "#### 2. Calculating TF-IDF\n",
    "\n",
    "The TF-IDF score for a term in a document is calculated as:\n",
    "$\n",
    "\\text{TF-IDF} = \\text{TF} \\times \\text{IDF}\n",
    "$\n",
    "The result gives a higher weight to terms that are frequent in a specific document but not frequent across all documents in the corpus.\n",
    "\n",
    "#### 3. Example of TF-IDF Calculation\n",
    "\n",
    "Suppose we have three documents:\n",
    "\n",
    "- Doc 1: \"The cat in the hat.\"\n",
    "- Doc 2: \"The cat is very fast.\"\n",
    "- Doc 3: \"A fast cat is in the hat.\"\n",
    "\n",
    "For each term, we would calculate TF for each document, IDF across all documents, and then multiply these values to get the TF-IDF score for each term in each document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e149b0a-f1e5-47af-a226-1d7c688461e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.34676577 0.         0.44652407 0.44652407 0.         0.69353155\n",
      "  0.        ]\n",
      " [0.34957775 0.45014501 0.         0.         0.45014501 0.34957775\n",
      "  0.59188659]\n",
      " [0.34035465 0.43826859 0.43826859 0.43826859 0.43826859 0.34035465\n",
      "  0.        ]]\n",
      "['cat' 'fast' 'hat' 'in' 'is' 'the' 'very']\n"
     ]
    }
   ],
   "source": [
    "#### 4. Using TF-IDF in Scikit-Learn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat in the hat.\",\n",
    "    \"The cat is very fast.\",\n",
    "    \"A fast cat is in the hat.\"\n",
    "]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Show the TF-IDF matrix\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Get feature names (terms)\n",
    "print(vectorizer.get_feature_names_out()) # the dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f469b8c-6ca9-4f30-b2b8-19bbc4251cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateResponse(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response) # append the inquiry to the sent_tokens \n",
    "    TfidfVec = TfidfVectorizer(tokenizer=Normalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    # Calculate the similarity of the inquiry with any of the sentences (vectors)\n",
    "    vals = linear_kernel(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2] # this is the index of the sentence most similar to the inquiry \n",
    "    flat = vals.flatten() # shape (1,170)--> (170)\n",
    "    flat.sort() # sort based on an ascending order \n",
    "    req_tfidf = flat[-2]# the first most similar is the document itself \n",
    "    if(req_tfidf==0) or \"tell me about\" in user_response: # meaning that no similariy is found or the prompt is totally unrelated to the text data  \n",
    "        print(\"Checking Wikipedia\")\n",
    "        if user_response:# if the user_response passed is not emtpy\n",
    "            robo_response = wikipedia_data(user_response)\n",
    "            return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx] # return the related sentence to teh inquiry \n",
    "        return robo_response\n",
    "\n",
    "def wikipedia_data(input):\n",
    "    reg_ex = re.search('tell me about (.*)', input) \n",
    "    try:\n",
    "        if reg_ex: # if the reg_ex is not empty \n",
    "            topic = reg_ex.group(1) #extract the topic that the user has asked about \n",
    "        else: \n",
    "            topic = input\n",
    "        wiki = wikipedia.summary(topic, sentences = 3)\n",
    "        return wiki\n",
    "        \n",
    "    except Exception as e:\n",
    "            print(\"No content has been found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65db8095-5d2d-44bf-89cc-d14fdfa4001d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'York is a cathedral city in North Yorkshire, England, with Roman origins, sited at the confluence of the rivers Ouse and Foss. It is the county town of Yorkshire. The city has many historic buildings and other structures, such as a minster, castle, and city walls.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "wikipedia.summary('York', sentences = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80459336-66da-4b37-9e67-eb517ed2aa71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a1a9bcf-ec35-48f6-a751-d4a4afa21c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "inquiry = 'What is the equality act, 2010 of the United Kingdom?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "955299b5-90a3-4b41-ba29-0d6d1e5e8f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens.append(inquiry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dac3d35-37c7-4fb9-85c5-f65e18f5aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVec = TfidfVectorizer(tokenizer=Normalize, stop_words='english')\n",
    "tfidf = TfidfVec.fit_transform(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cdd9771-1411-4f50-bf6b-b51524dc3570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169, 1027)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40d4a0be-7eba-4a01-8b1d-8d57867d5b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1027 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93d91a98-6e38-41cf-a106-765b78388797",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = linear_kernel(tfidf[-1], tfidf) # find the similarity of every sentence with the inquiry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5bb8c0f3-d549-4411-ba38-b7fb9e5657d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07651511, 0.18818161, 0.        ,\n",
       "        0.4672911 , 0.03159562, 0.        , 0.        , 0.        ,\n",
       "        0.06151453, 0.05121481, 0.05494332, 0.        , 0.        ,\n",
       "        0.07776611, 0.        , 0.07161565, 0.        , 0.        ,\n",
       "        0.        , 0.05805943, 0.0813911 , 0.        , 0.        ,\n",
       "        0.        , 0.0331519 , 0.        , 0.        , 0.        ,\n",
       "        0.07070032, 0.05155054, 0.        , 0.        , 0.        ,\n",
       "        0.03358599, 0.02507002, 0.        , 0.02691169, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.03257163, 0.03147009,\n",
       "        0.02684319, 0.        , 0.1040516 , 0.        , 0.        ,\n",
       "        0.        , 0.0443987 , 0.06375502, 0.        , 0.        ,\n",
       "        0.05316166, 0.        , 0.        , 0.        , 0.04418997,\n",
       "        0.        , 0.05854444, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.03616975, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals #array of 170 indecies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a4d4241-f835-435b-8c75-0ab920ec2cf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118,\n",
       "        119, 120, 121, 122, 123, 106, 124, 105, 103,  86,  87,  88,  89,\n",
       "         90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102,\n",
       "        104, 125, 126, 127, 150, 151, 152, 153, 154, 155, 156, 157, 158,\n",
       "        159, 160, 161, 163, 164, 165, 166, 167, 149, 148, 147, 146, 128,\n",
       "        129, 130, 131, 132, 133, 134, 135,  85, 136, 138, 139, 140, 141,\n",
       "        142, 143, 144, 145, 137, 168,  84,  82,  19,  20,  21,  24,  27,\n",
       "         28,  18,  29,  34,  36,  38,  39,  83,  43,  33,  44,  17,  15,\n",
       "          1,   2,   3,   4,   5,   6,  16,   7,   9,  10,  11,  12,  13,\n",
       "         14,   8,  45,  40,  62,  60,  57,  78,  47,  66,  61,  77,  80,\n",
       "         59,  54,  53,  52,  69,  70,  73,  74,  49,  48,  68,  76,  56,\n",
       "         65,  58,  64,  26,  63,  46,  55, 162,  79,  71,  31,  51,  75,\n",
       "         32,  41,  81,  30,  72,  50,  37,  22,  35,  42,  67,  23,  25,\n",
       "        169]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx=vals.argsort()\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa3172ab-f89c-47af-93f7-8c9d3a9bc9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118,\n",
       "       119, 120, 121, 122, 123, 106, 124, 105, 103,  86,  87,  88,  89,\n",
       "        90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102,\n",
       "       104, 125, 126, 127, 150, 151, 152, 153, 154, 155, 156, 157, 158,\n",
       "       159, 160, 161, 163, 164, 165, 166, 167, 149, 148, 147, 146, 128,\n",
       "       129, 130, 131, 132, 133, 134, 135,  85, 136, 138, 139, 140, 141,\n",
       "       142, 143, 144, 145, 137, 168,  84,  82,  19,  20,  21,  24,  27,\n",
       "        28,  18,  29,  34,  36,  38,  39,  83,  43,  33,  44,  17,  15,\n",
       "         1,   2,   3,   4,   5,   6,  16,   7,   9,  10,  11,  12,  13,\n",
       "        14,   8,  45,  40,  62,  60,  57,  78,  47,  66,  61,  77,  80,\n",
       "        59,  54,  53,  52,  69,  70,  73,  74,  49,  48,  68,  76,  56,\n",
       "        65,  58,  64,  26,  63,  46,  55, 162,  79,  71,  31,  51,  75,\n",
       "        32,  41,  81,  30,  72,  50,  37,  22,  35,  42,  67,  23,  25,\n",
       "       169])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals.argsort()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77dacbc8-0bf6-4338-9ec1-94927251ba30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx=vals.argsort()[0][-2]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53a4d161-ca79-4f28-b534-49d33982f93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat = vals.flatten()\n",
    "flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59596b0b-29fb-4f5f-a49c-fe738bcfd61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.02507002, 0.02684319, 0.02691169,\n",
       "       0.03147009, 0.03159562, 0.03257163, 0.0331519 , 0.03358599,\n",
       "       0.03616975, 0.04418997, 0.0443987 , 0.05121481, 0.05155054,\n",
       "       0.05316166, 0.05494332, 0.05805943, 0.05854444, 0.06151453,\n",
       "       0.06375502, 0.07070032, 0.07161565, 0.07651511, 0.07776611,\n",
       "       0.0813911 , 0.1040516 , 0.18818161, 0.4672911 , 1.        ])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat.sort() # sort based on an ascending order \n",
    "flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545476cb-4e78-4d25-a008-e783e749d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_tfidf = flat[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16f7bdc8-93b0-403b-8eaa-c3bdf2d67f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python programming'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "input = \"tell me about Python programming\"\n",
    "reg_ex = re.search('tell me about (.*)', input)\n",
    "reg_ex.group(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "044e3ec3-219a-4d34-85fe-caf4200347ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Shirin's first ChatBot! How can I help you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " What is the equality act, 2010 of the United Kingdom?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatterbot : the equality act, 2010 of the united kingdom prohibits discrimination and mandates equal treatment in matters of employment as well as private and public services irrespective of race, age, sex, religion or disability .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " What are the main focus areas for use of analytics in the Asia-Pacific countries?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatterbot : compensation and benefits, talent acquisition, talent development and productivity are the established focus areas for use of analytics in the asia-pacific region.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " What article in the Indian Constitution talks about the right of Indian citizens?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatterbot : adherence to the rule of equality in public employment is a being feature of indian constitution and the rule of law is its core, the court cannot disable itself from making an order inconsistent with article 14 and 16 of the indian constitution.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Tell me about York University \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatterbot : Checking Wikipedia\n",
      "York University (French: Université York), also known as YorkU or simply YU,  is a public research university in Toronto, Ontario, Canada. It is Canada's third-largest university, and it has approximately 53,500 students, 7,000 faculty and staff, and over 375,000 alumni worldwide. It has 11 faculties, including the Lassonde School of Engineering, Schulich School of Business, Osgoode Hall Law School, Glendon College, and 32 research centres.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Thank you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatterbot : You are welcome..\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"This is Shirin's first ChatBot! How can I help you today?\")\n",
    "while(flag==True):\n",
    "    # Ask for an input \n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response not in ['bye','shutdown','exit', 'quit']):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"Chatterbot : You are welcome..\")\n",
    "        else:\n",
    "            if(welcome(user_response)!=None):# Meaning that a greeting is passed to the bot\n",
    "                print(\"Chatterbot : \"+welcome(user_response))\n",
    "            else:\n",
    "                print(\"Chatterbot : \",end=\"\")\n",
    "                print(generateResponse(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"Chatterbot : Bye!!! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5807755-7028-4542-b1f9-01266fe4f4ef",
   "metadata": {},
   "source": [
    "Suggested pathways: \n",
    "* Explore how this model performs while equipped with attention models and word embeddings\n",
    "* On a bigger scale, we can create a text summarization tool that takes in any pdf file and provides a summary of this pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1a3177-14a0-46ef-bc7b-71a6a198c27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee9f4684-0545-4ae7-90e5-1f202883f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "# Sample list of words with non-ASCII characters\n",
    "word_token = ['résumé', 'café', 'façade', 'niño', 'jalapeño']\n",
    "\n",
    "# Initialize an empty list to store the cleaned words\n",
    "new_words = []\n",
    "\n",
    "# Loop through each word in the list `word_token`\n",
    "for word in word_token:\n",
    "    # Normalize, encode to ASCII, and decode back\n",
    "    new_word = unicodedata.normalize('NFKD', word) \\\n",
    "        .encode('ascii', 'ignore')  # Remove non-ASCII characters\n",
    "    \n",
    "    # Decode back to UTF-8 and append to new_words list\n",
    "    new_word = new_word.decode('utf-8', 'ignore')\n",
    "    \n",
    "    # Append cleaned word to the list\n",
    "    new_words.append(new_word)\n",
    "\n",
    "# Output the new list of words\n",
    "print(new_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd4a1e-b164-48c1-b506-357a4b6f9e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
